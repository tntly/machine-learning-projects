# Malware Family Classification using API Call Sequences Report

> Author: Tien Ly  
> CS 271: Topics in Machine Learning - Spring 2025 at San Jose State University

## Table of Contents

- [1. Introduction](#1-introduction)
- [2. Dataset](#2-dataset)
- [3. Experiments](#3-experiments)
    - [Embedding Methods](#embedding-methods)
        - [Word2Vec](#word2vec)
        - [BERT](#bert)
    - [Traditional Classifier Implementation](#traditional-classifier-implementation)
    - [CNN Implementation with Word2Vec](#cnn-implementation-with-word2vec)
    - [CNN Implementation with BERT](#cnn-implementation-with-bert)
    - [Hyperparameter Tuning and Model Comparison](#hyperparameter-tuning-and-model-comparison)
- [4. Results](#4-results)
    - [Word2Vec and Traditional Classifiers](#word2vec-and-traditional-classifiers)
    - [BERT and Traditional Classifiers](#bert-and-traditional-classifiers)
    - [Word2Vec and CNN](#word2vec-and-cnn)
    - [BERT and CNN](#bert-and-cnn)
    - [Summary of Class-wise Performance](#summary-of-class-wise-performance)
    - [Hyperparameter Tuning and Model Summary](#hyperparameter-tuning-and-model-summary)
        - [Word2Vec and Random Forest Tuning](#word2vec-and-random-forest-tuning)
        - [BERT and CNN Tuning](#bert-and-cnn-tuning)
        - [Model Comparison](#model-comparison)
- [5. Conclusion](#5-conclusion)

## 1. Introduction

In my project, I explored a hybrid machine learning approach for classifying malware into distinct families by analyzing sequences of API calls recorded during dynamic execution. My motivation stemmed from the observation that API call sequences, much like natural language, carry contextual and semantic patterns that can be learned through embedding techniques. By treating API calls as tokens in a sequence, I used Word2Vec and BERT to convert these sequences into numerical representations. I then utilized these embeddings as features for classification algorithms, including XGBoost, Random Forest (RF), and Convolutional Neural Networks (CNN).

My primary goal was to evaluate whether embeddings of API call sequences could effectively capture behavior patterns unique to each malware family and improve the accuracy of multi-class classification. Through a series of experiments comparing traditional machine learning models and deep learning architectures, I assessed the effectiveness of each combination of embedding technique and classifier. My results demonstrated that API call sequences contain rich semantic information and that embedding-based approaches offer a promising direction for enhancing malware classification accuracy.

## 2. Dataset

The dataset used for this study consists of Windows executable malware samples, collected and analyzed through dynamic analysis to extract their behavioral characteristics. These samples were sourced from a prior Master's thesis project titled "Malware Classification using API Call Information and Word Embeddings," authored by Sahil Aggarwal at San Jose State University in Spring 2023. In total, the dataset includes 492 samples and spans seven distinct malware families: Adload, Bancos, Onlinegames, VBInject, Vundo, Winwebsec, and Zwangi. Each sample underwent execution in a sandboxed environment, and the resulting logs were analyzed to extract API call sequences that reflect the malware's runtime behavior.

Dynamic analysis was performed using Buster Sandbox Analyzer (BSA), an open-source tool that captures detailed behavioral data such as API calls, file system changes, and network activities. To ensure safe execution of malicious binaries, the analysis was conducted within an isolated virtual environment provided by Sandboxie. Each sample was run individually in this environment, and the API call logs generated by BSA were collected and stored. Preprocessing of these raw logs involved extracting only the API call names and removing any metadata, such as file paths or arguments, to yield clean sequences suitable for downstream embedding. For example, a cleaned sample may include a sequence such as:

```
QuerySystemInformation
QuerySystemInformation
QueryProcessInformation
CreateThread
ResumeThread
OpenProcessToken
VirtualAllocEx
VirtualAllocEx
VirtualAllocEx
VirtualAllocEx
...
```

I identified a total of 79 unique API calls across the dataset, with VirtualAllocEx being the most frequently observed. These sequences, once cleaned, were treated as input sentences for word embedding models. The distribution of samples across malware families is summarized in the following table.

*Table 1. Number of Samples per Malware Family*
|    Family   | Sample Count |
| :---------: | :----------: |
|    Adload   |      70      |
|    Bancos   |      71      |
| Onlinegames |      70      |
|   VBInject  |      70      |
|    Vundo    |      71      |
|  Winwebsec  |      70      |
|    Zwangi   |      70      |

These API call sequences served as the foundation for all subsequent embedding and classification experiments I conducted in this project.

## 3. Experiments

I designed and conducted experiments to evaluate how well different combinations of embedding methods and classifiers can distinguish between malware families based on sequences of API calls. I explored two word embedding strategies (Word2Vec and BERT) and applied them to both traditional machine learning models and deep learning models. The classifiers used were Random Forest (RF), XGBoost, and Convolutional Neural Networks (CNN). Each embedding-classifier combination was implemented and tested in a dedicated Jupyter notebook to ensure modularity and reproducibility. In total, I created four notebooks to support the complete experimental workflow.

### Embedding Methods

#### Word2Vec

To generate Word2Vec embeddings, I first tokenized each malware sample’s API call sequence into a list of API call names. I trained a Word2Vec model using the `gensim` library, setting the vector size to 100, window size to 5, and `min_count` to 1 to ensure even infrequent API calls were embedded. The resulting Word2Vec model captured local contextual information within API call sequences. For each sample, I calculated the average of the API call vectors to obtain a fixed-length 100-dimensional embedding.

#### BERT

For the BERT-based embedding approach, I used the `bert-base-uncased` model from Hugging Face Transformers. I concatenated each sample's API calls into a single space-separated string and passed it through BERT’s tokenizer. I used the [CLS] token from the last hidden layer as the representation for the full API call sequence. This method allowed me to leverage pretrained contextual embeddings, although I did not fine-tune BERT during training. The extracted embeddings were stored in the DataFrame for subsequent classification.

### Traditional Classifier Implementation

In the `malware-ml.ipynb` notebook, I trained and evaluated Random Forest and XGBoost classifiers using both Word2Vec and BERT embeddings as input features. I used `scikit-learn` for Random Forest and `xgboost` for the XGBoost implementation. Labels were encoded using `LabelEncoder`, and I split the data into training and test sets with a 80/20 split. 

For Random Forest, I initialized the model with 100 trees and allowed unlimited depth in the untuned experiments. For XGBoost, I used `multi:softprob` as the objective to handle multi-class classification and used a learning rate of 0.1 with 100 boosting rounds. I evaluated the models using accuracy, precision, recall, F1-score, and confusion matrix metrics. These metrics were computed using `classification_report` and `confusion_matrix` from `scikit-learn`.

### CNN Implementation with Word2Vec

In the `malware-word2vec-cnn.ipynb` notebook, I implemented a CNN classifier using PyTorch. I first trained a Word2Vec model as described above, then mapped each API call to its corresponding token ID using the trained Word2Vec vocabulary. I padded the sequences to the maximum length in the dataset and created PyTorch tensors for the input features and labels.

The CNN architecture consisted of an embedding layer initialized with the pretrained Word2Vec vectors and frozen during training. I applied three parallel 1D convolution layers with kernel sizes of 3, 4, and 5, followed by global max pooling and concatenation. The output was passed through a dropout layer (rate 0.5) and a fully connected layer for classification. I trained the model using the Adam optimizer and CrossEntropyLoss for up to 50 epochs, using early stopping with a patience of 20 based on validation accuracy. Training, validation, and test splits were 80%, 10%, and 10%, respectively.

### CNN Implementation with BERT

The `malware-bert-cnn.ipynb` notebook extended the CNN architecture to BERT embeddings. I tokenized each API call sequence using the BERT tokenizer and obtained input IDs and attention masks. These were padded and converted into PyTorch `TensorDataset` objects. 

Using the pretrained BERT model, I extracted the contextual embeddings of the tokenized input and used the last hidden states as input to the CNN. The CNN applied multiple 1D convolution layers with different filter sizes (3, 4, 5), followed by max pooling, dropout, and a final classification layer. I trained the model using the same strategy as the Word2Vec-based CNN, with an initial learning rate of 0.01 and early stopping to prevent overfitting.

### Hyperparameter Tuning and Model Comparison

Finally, I performed hyperparameter tuning in the `malware-summary.ipynb` notebook. For the Word2Vec + Random Forest model, I used `GridSearchCV` to evaluate 45 different parameter combinations involving the number of trees, maximum depth, and minimum samples required to split a node. For the BERT + CNN model, I manually tested 27 configurations with varying numbers of filters, dropout rates, and learning rates. The best-performing hyperparameters were selected based on validation accuracy and then used to retrain and evaluate the models on the test set.

All performance metrics, including training histories, classification reports, and confusion matrices, were saved to disk for comparison and reproducibility.

*Table 2. Grid Search Hyperparameters for Word2Vec + Random Forest*
| Hyperparameter      | Values Tested                    |
|:-------------------:|:--------------------------------:|
| n_estimators        | 100, 200, 300, 400, 500          |
| max_depth           | 10, 20, 30                       |
| min_samples_split   | 2, 5, 10                         |

*Table 3. Manual Search Hyperparameters for BERT + CNN*
| Hyperparameter   | Values Tested          |
|:----------------:|:----------------------:|
| num_filters      | 50, 100, 150           |
| dropout_rate     | 0.3, 0.5, 0.7          |
| learning_rate    | 0.1, 0.01, 0.001       |

## 4. Results

After training and evaluating all combinations of embedding methods and classifiers, I found that all models achieved relatively high accuracy, with some combinations outperforming others in terms of class-wise precision and recall. The BERT + CNN model performed best overall, while the Word2Vec + Random Forest model provided competitive results with lower computational requirements.

### Word2Vec and Traditional Classifiers

The Word2Vec + Random Forest model achieved a test accuracy of 87.00%. This model consistently classified families such as Adload and Zwangi with high precision and recall. However, it frequently misclassified samples from the Bancos, Onlinegames, VBInject, and Winwebsec families. Despite performing a grid search over 45 parameter combinations, the tuned model did not significantly outperform the initial version, indicating that the default hyperparameters were already close to optimal.

<div align="center">
  <img src="img/w2v-rf-report.png" width="400" alt="Classification Report of Word2Vec-RF">
  <p><em>Figure 1. Classification Report of Word2Vec-RF</em></p>
</div>

<div align="center">
  <img src="img/w2v-rf-matrix.png" width="500" alt="Confusion Matrix of Word2Vec-RF">
  <p><em>Figure 2. Confusion Matrix of Word2Vec-RF</em></p>
</div>

For the Word2Vec + XGBoost model, performance was similar. Accuracy was slightly lower than the Random Forest counterpart, and the model showed difficulty distinguishing between Onlinegames and Winwebsec samples. This suggests that while Word2Vec captures useful features, more complex classifiers do not necessarily yield better results without improved embeddings.

<div align="center">
  <img src="img/w2v-xgboost-report.png" width="400" alt="Classification Report of Word2Vec-XGBoost">
  <p><em>Figure 3. Classification Report of Word2Vec-XGBoost</em></p>
</div>

<div align="center">
  <img src="img/w2v-xgboost-matrix.png" width="500" alt="Confusion Matrix of Word2Vec-XGBoost">
  <p><em>Figure 4. Confusion Matrix of Word2Vec-XGBoost</em></p>
</div>

### BERT and Traditional Classifiers

The BERT + Random Forest model showed a modest improvement in recall for difficult classes such as Bancos. However, this gain came at the cost of lower precision, particularly for the Winwebsec family. This trade-off reflects how the contextual embeddings produced by BERT can better represent ambiguous sequences but may introduce overlap between semantically similar families.

<div align="center">
  <img src="img/bert-rf-report.png" width="400" alt="Classification Report of BERT-RF">
  <p><em>Figure 5. Classification Report of BERT-RF</em></p>
</div>

<div align="center">
  <img src="img/bert-rf-matrix.png" width="500" alt="Confusion Matrix of BERT-RF">
  <p><em>Figure 6. Confusion Matrix of BERT-RF</em></p>
</div>

The BERT + XGBoost model performed comparably, although slightly less consistently. As with the Word2Vec-based models, XGBoost did not significantly outperform Random Forest, which may be due to the relatively small size of the dataset and limited complexity of the features after averaging.

<div align="center">
  <img src="img/bert-xgboost-report.png" width="400" alt="Classification Report of BERT-XGBoost">
  <p><em>Figure 7. Classification Report of BERT-XGBoost</em></p>
</div>

<div align="center">
  <img src="img/bert-xgboost-matrix.png" width="500" alt="Confusion Matrix of BERT-XGBoost">
  <p><em>Figure 8. Confusion Matrix of BERT-XGBoost</em></p>
</div>

### Word2Vec and CNN

Using Word2Vec embeddings with a CNN classifier, I achieved a test accuracy of 86.00% and a test loss of 0.5454. This model exhibited stable convergence during training, with minimal signs of overfitting due to the use of dropout and early stopping. Class-wise performance was strong for Adload, Winwebsec, and Zwangi. However, recall remained low for Bancos and Onlinegames, reflecting similar weaknesses observed in the traditional models. The fixed embedding vectors and frozen embedding layer may have limited the CNN’s ability to adjust representations dynamically.

<div align="center">
  <img src="img/w2v-cnn-report.png" width="400" alt="Classification Report of Word2Vec-CNN">
  <p><em>Figure 9. Classification Report of Word2Vec-CNN</em></p>
</div>

<div align="center">
  <img src="img/w2v-cnn-matrix.png" width="500" alt="Confusion Matrix of Word2Vec-CNN">
  <p><em>Figure 10. Confusion Matrix of Word2Vec-CNN</em></p>
</div>

<div align="center">
  <img src="img/w2v-cnn-perf.png" width="700" alt="Accuracy and Loss Plots of Word2Vec-CNN">
  <p><em>Figure 11. Accuracy and Loss Plots of Word2Vec-CNN</em></p>
</div>

### BERT and CNN

The BERT + CNN model achieved the highest test accuracy of 90.00% and the lowest loss at 0.3806. This model demonstrated the strongest class-wise performance, achieving perfect precision and recall for Winwebsec and Zwangi. The lowest recall was observed for VBInject (0.71), indicating some confusion with similar families. The CNN architecture effectively leveraged BERT’s contextual embeddings, and training metrics showed smooth convergence with minimal validation loss fluctuation.

To further optimize this model, I conducted manual tuning by varying the number of filters, dropout rate, and learning rate. The best validation accuracy achieved during tuning was 93.88%, using 50 filters, a dropout rate of 0.5, and a learning rate of 0.001. However, the test accuracy for the tuned model was 86.00%, slightly lower than the untuned version, possibly due to overfitting to the validation set.

<div align="center">
  <img src="img/bert-cnn-report.png" width="400" alt="Classification Report of BERT-CNN">
  <p><em>Figure 12. Classification Report of BERT-CNN</em></p>
</div>

<div align="center">
  <img src="img/bert-cnn-matrix.png" width="500" alt="Confusion Matrix of BERT-CNN">
  <p><em>Figure 13. Confusion Matrix of BERT-CNN</em></p>
</div>

<div align="center">
  <img src="img/bert-cnn-perf.png" width="700" alt="Accuracy and Loss Plots of BERT-CNN">
  <p><em>Figure 14. Accuracy and Loss Plots of BERT-CNN</em></p>
</div>

### Summary of Class-wise Performance

Across all models, certain malware families such as Adload and Zwangi were consistently classified with high confidence, regardless of embedding or model type. In contrast, Bancos, Onlinegames, and VBInject frequently exhibited lower recall or precision. These misclassifications suggest overlapping API behavior patterns between some families and highlight the need for more expressive or fine-tuned embeddings. Confusion matrices revealed that most errors were not random but concentrated among a few families. This insight may inform future work in hierarchical classification or embedding enhancement strategies.

### Hyperparameter Tuning and Model Summary

After evaluating all initial model configurations, I identified two combinations for further optimization: Word2Vec + Random Forest and BERT + CNN. These models were selected based on their accuracy. I performed systematic hyperparameter tuning for both to explore whether additional performance gains could be achieved. I also compiled results from all experiments to compare their accuracy and class-wise behavior in a consistent format.

#### Word2Vec and Random Forest Tuning

To optimize the Word2Vec + Random Forest model, I conducted a grid search using `GridSearchCV` from `scikit-learn` and tested 45 hyperparameter combinations. The best-performing configuration used 100 trees, a maximum depth of 20, and a minimum samples split of 2. However, this tuned model did not result in a significant performance improvement over the untuned version, with the test accuracy remaining at 87.00%.

<div align="center">
  <img src="img/w2v-rf-report-tuned.png" width="400" alt="Classification Report of Word2Vec-RF (Tuned)">
  <p><em>Figure 15. Classification Report of Word2Vec-RF (Tuned)</em></p>
</div>

<div align="center">
  <img src="img/w2v-rf-matrix-tuned.png" width="500" alt="Confusion Matrix of Word2Vec-RF (Tuned)">
  <p><em>Figure 16. Confusion Matrix of Word2Vec-RF (Tuned)</em></p>
</div>

#### BERT and CNN Tuning

The BERT + CNN model was manually tuned using a grid of 27 parameter configurations. The best validation performance was achieved using 50 filters, a dropout rate of 0.5, and a learning rate of 0.001, which resulted in a validation accuracy of 93.88%. Interestingly, this model did not surpass the original untuned model’s test performance, as the final test accuracy dropped slightly to 86.00%, suggesting mild overfitting to the validation set.

<div align="center">
  <img src="img/bert-cnn-report-tuned.png" width="400" alt="Classification Report of BERT-CNN (Tuned)">
  <p><em>Figure 17. Classification Report of BERT-CNN (Tuned)</em></p>
</div>

<div align="center">
  <img src="img/bert-cnn-matrix-tuned.png" width="500" alt="Confusion Matrix of BERT-CNN (Tuned)">
  <p><em>Figure 18. Confusion Matrix of BERT-CNN (Tuned)</em></p>
</div>

<div align="center">
  <img src="img/bert-cnn-perf-tuned.png" width="700" alt="Accuracy and Loss Plots of BERT-CNN (Tuned)">
  <p><em>Figure 19. Accuracy and Loss Plots of BERT-CNN (Tuned)</em></p>
</div>

#### Model Comparison

I summarized the results from all eight model combinations to compare their classification accuracy and robustness. All models achieved at least 86% test accuracy, demonstrating the effectiveness of embedding-based representations for malware classification. The BERT + CNN model without tuning achieved the highest overall accuracy (90.00%), while the Word2Vec + CNN and Random Forest models showed strong and reliable performance with lower computational cost.

*Table 4. Summary of Test Accuracy Across All Models*
| Embedding | Classifier     | Tuned | Test Accuracy  |
|:---------:|:--------------:|:-----:|:--------------:|
| BERT      | CNN            | No    | **90.00%**     |
| BERT      | CNN            | Yes   | 86.00%         |
| BERT      | Random Forest  | No    | 86.00%         |
| BERT      | XGBoost        | No    | 86.00%         |
| Word2Vec  | CNN            | No    | 86.00%         |
| Word2Vec  | Random Forest  | No    | 87.00%         |
| Word2Vec  | Random Forest  | Yes   | 87.00%         |
| Word2Vec  | XGBoost        | No    | 86.00%         |

The results suggest that BERT embeddings combined with CNNs provide the best classification accuracy, although gains diminish with tuning. Word2Vec remains a strong alternative for lower-resource scenarios. The classification performance is consistent across models for certain families, while others remain difficult to distinguish due to overlapping behavioral patterns in their API call sequences. Future work may benefit from fine-tuning BERT, using sequence models such as RNNs or Transformers, or incorporating additional contextual metadata from the logs.

## 5. Conclusion

In this project, I investigated the use of word embeddings and machine learning models for classifying malware families based on API call sequences obtained from dynamic analysis. The experiments evaluated the effectiveness of Word2Vec and BERT embeddings when combined with both traditional classifiers (Random Forest and XGBoost) and deep learning models such as Convolutional Neural Networks.

Overall, all embedding–classifier combinations achieved strong performance, with test accuracies ranging from 86% to 90%. The best-performing model was the BERT + CNN architecture, which achieved a test accuracy of 90% without fine-tuning. This model benefited from BERT’s ability to capture contextual semantics and the CNN’s capability to extract local discriminative patterns. However, performance on certain malware families, such as VBInject, remained lower due to overlapping behavioral features and potential data imbalance. The Word2Vec + Random Forest model offered a compelling balance between performance and simplicity, reaching 87% accuracy with minimal training time and high interpretability. Despite extensive hyperparameter tuning, the performance gains from tuning were marginal for both this model and the BERT + CNN model, suggesting that default settings were already near optimal for the given dataset. The CNN models trained on Word2Vec embeddings also showed promise. While slightly less accurate than their BERT-based counterparts, they still outperformed traditional models in some cases and demonstrated that CNNs can successfully learn useful representations from API sequences using even static embeddings.

Based on these findings, I conclude that both Word2Vec and BERT embeddings are effective for this classification task. For fast and interpretable solutions, Word2Vec + Random Forest provides a strong baseline. For applications where model accuracy is critical and computational resources allow, the BERT + CNN architecture is the most effective choice. Future work could explore fine-tuning BERT to better align with the malware domain, incorporating attention mechanisms after CNN layers, or using larger and more diverse datasets to improve generalization for harder-to-classify families. These enhancements may further close the gap in precision and recall for families with overlapping behavior, ultimately improving the robustness of malware detection systems.
